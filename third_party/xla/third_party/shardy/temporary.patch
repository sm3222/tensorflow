diff --git a/shardy/dialect/sdy/transforms/export/explicit_reshards_util.cc b/shardy/dialect/sdy/transforms/export/explicit_reshards_util.cc
index 2347b8a..d43c295 100644
--- a/shardy/dialect/sdy/transforms/export/explicit_reshards_util.cc
+++ b/shardy/dialect/sdy/transforms/export/explicit_reshards_util.cc
@@ -268,8 +268,8 @@ struct FactorAxesCandidate {
   FactorAxesCandidate() = default;
 
   // Multi-level comparison.
-  // 0. totalSourceTensorSize
-  // 1. communicationCost
+  // 0. communicationCost
+  // 1. totalSourceTensorSize
   // 2. factorTypePrecedence
   // 3. largestSourceTensorSize
   // 4. shardingSize
@@ -277,7 +277,7 @@ struct FactorAxesCandidate {
   bool operator<(const FactorAxesCandidate& rhs) const {
     auto makeComparisonTuple = [](const FactorAxesCandidate& candidate) {
       return std::make_tuple(
-          candidate.totalSourceTensorSize, -candidate.communicationCost,
+          -candidate.communicationCost, candidate.totalSourceTensorSize,
           candidate.factorTypePrecedence, candidate.largestSourceTensorSize,
           candidate.shardingSize, candidate.factorAxes);
     };
diff --git a/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards/concatenate.mlir b/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards/concatenate.mlir
index 92d6f63..8fa12c7 100644
--- a/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards/concatenate.mlir
+++ b/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards/concatenate.mlir
@@ -13,8 +13,8 @@ func.func @concatenate_single_input(%arg0: tensor<4x32x256xf32> {sdy.sharding =
 
 // CHECK-LABEL: func @concatenate
 func.func @concatenate(%arg0: tensor<4x32x256xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}, {}]>}, %arg1: tensor<4x48x256xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"y"}, {}, {}]>}) -> tensor<4x80x256xf32> {
-  // CHECK: %[[RESHARD1:.*]] = sdy.reshard %arg0 <@mesh, [{"y"}, {}, {}]> : tensor<4x32x256xf32>
-  // CHECK-NEXT: %[[CONCATENATE:.*]] = stablehlo.concatenate %[[RESHARD1]], %arg1, dim = 1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"y"}, {}, {}]>]>} : (tensor<4x32x256xf32>, tensor<4x48x256xf32>) -> tensor<4x80x256xf32>
+  // CHECK: %[[RESHARD1:.*]] = sdy.reshard %arg1 <@mesh, [{"x"}, {}, {}]> : tensor<4x48x256xf32>
+  // CHECK-NEXT: %[[CONCATENATE:.*]] = stablehlo.concatenate %arg0, %[[RESHARD1]], dim = 1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"x"}, {}, {}]>]>} : (tensor<4x32x256xf32>, tensor<4x48x256xf32>) -> tensor<4x80x256xf32>
   // CHECK-NEXT: %[[RESHARD2:.*]] = sdy.reshard %[[CONCATENATE]] <@mesh, [{}, {}, {}]> : tensor<4x80x256xf32>
   // CHECK-NEXT: return %[[RESHARD2]] : tensor<4x80x256xf32>
   %0 = stablehlo.concatenate %arg0, %arg1, dim = 1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {}]>]>} : (tensor<4x32x256xf32>, tensor<4x48x256xf32>) -> tensor<4x80x256xf32>
@@ -64,10 +64,11 @@ func.func @concatenate_operands_are_from_slices_of_the_same_tensor(%arg0: tensor
 func.func @concatenate_operands_are_results_of_slices_different_shardings_on_permutation_dim_with_equal_counts(%arg0: tensor<4x40x256xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}, {}]>}, %arg1: tensor<4x60x256xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"y"}, {}]>}) -> (tensor<4x80x256xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"x"}, {}]>}) {
   %0 = stablehlo.slice %arg0 [0:4, 0:32, 0:256] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"x"}, {}, {}]>]>} : (tensor<4x40x256xf32>) -> tensor<4x32x256xf32>
   %1 = stablehlo.slice %arg1 [0:4, 0:48, 0:256] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"y"}, {}]>]>} : (tensor<4x60x256xf32>) -> tensor<4x48x256xf32>
-  // CHECK: %[[RESHARD1:.*]] = sdy.reshard %0 <@mesh, [{}, {"x"}, {}]> : tensor<4x32x256xf32>
-  // CHECK-NEXT: %[[RESHARD2:.*]] = sdy.reshard %1 <@mesh, [{}, {"x"}, {}]> : tensor<4x48x256xf32>
-  // CHECK-NEXT: %[[CONCATENATE:.*]] = stablehlo.concatenate %[[RESHARD1]], %[[RESHARD2]], dim = 1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"x"}, {}]>]>} : (tensor<4x32x256xf32>, tensor<4x48x256xf32>) -> tensor<4x80x256xf32>
-  // CHECK-NEXT: return %[[CONCATENATE:.*]] : tensor<4x80x256xf32>
+  // CHECK: %[[RESHARD1:.*]] = sdy.reshard %0 <@mesh, [{"x"}, {"y"}, {}]> : tensor<4x32x256xf32>
+  // CHECK-NEXT: %[[RESHARD2:.*]] = sdy.reshard %1 <@mesh, [{"x"}, {"y"}, {}]> : tensor<4x48x256xf32>
+  // CHECK-NEXT: %[[CONCATENATE:.*]] = stablehlo.concatenate %[[RESHARD1]], %[[RESHARD2]], dim = 1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"x"}, {"y"}, {}]>]>} : (tensor<4x32x256xf32>, tensor<4x48x256xf32>) -> tensor<4x80x256xf32>
+  // CHECK-NEXT: %[[RESHARD3:.*]] = sdy.reshard %[[CONCATENATE]] <@mesh, [{}, {"x"}, {}]> : tensor<4x80x256xf32>
+  // CHECK-NEXT: return %[[RESHARD3]] : tensor<4x80x256xf32>
   %2 = stablehlo.concatenate %0, %1, dim = 1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [
 {}, {"x"}, {}]>]>} : (tensor<4x32x256xf32>, tensor<4x48x256xf32>) -> tensor<4x80x256xf32>
   return %2 : tensor<4x80x256xf32>
@@ -77,10 +78,11 @@ func.func @concatenate_operands_are_results_of_slices_different_shardings_on_per
 func.func @concatenate_operands_are_results_of_slices_different_shardings_on_permutation_dim_with_equal_counts_but_conflicting_on_batching_dim(%arg0: tensor<4x40x256xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"x":(2)2}, {}, {}]>}, %arg1: tensor<4x60x256xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"y"}, {}]>}) -> (tensor<4x80x256xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"x"}, {}]>}) {
   %0 = stablehlo.slice %arg0 [0:4, 0:32, 0:256] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"x":(2)2}, {}, {}]>]>} : (tensor<4x40x256xf32>) -> tensor<4x32x256xf32>
   %1 = stablehlo.slice %arg1 [0:4, 0:48, 0:256] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"y"}, {}]>]>} : (tensor<4x60x256xf32>) -> tensor<4x48x256xf32>
-  // CHECK: %[[RESHARD1:.*]] = sdy.reshard %0 <@mesh, [{}, {"x"}, {}]> : tensor<4x32x256xf32>
-  // CHECK-NEXT: %[[RESHARD2:.*]] = sdy.reshard %1 <@mesh, [{}, {"x"}, {}]> : tensor<4x48x256xf32>
-  // CHECK-NEXT: %[[CONCATENATE:.*]] = stablehlo.concatenate %[[RESHARD1]], %[[RESHARD2]], dim = 1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"x"}, {}]>]>} : (tensor<4x32x256xf32>, tensor<4x48x256xf32>) -> tensor<4x80x256xf32>
-  // CHECK-NEXT: return %[[CONCATENATE:.*]] : tensor<4x80x256xf32>
+  // CHECK: %[[RESHARD1:.*]] = sdy.reshard %0 <@mesh, [{"x":(2)2}, {"y"}, {}]> : tensor<4x32x256xf32>
+  // CHECK-NEXT: %[[RESHARD2:.*]] = sdy.reshard %1 <@mesh, [{"x":(2)2}, {"y"}, {}]> : tensor<4x48x256xf32>
+  // CHECK-NEXT: %[[CONCATENATE:.*]] = stablehlo.concatenate %[[RESHARD1]], %[[RESHARD2]], dim = 1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"x":(2)2}, {"y"}, {}]>]>} : (tensor<4x32x256xf32>, tensor<4x48x256xf32>) -> tensor<4x80x256xf32>
+  // CHECK-NEXT: %[[RESHARD3:.*]] = sdy.reshard %[[CONCATENATE]] <@mesh, [{}, {"x"}, {}]> : tensor<4x80x256xf32>
+  // CHECK-NEXT: return %[[RESHARD3]] : tensor<4x80x256xf32>
   %2 = stablehlo.concatenate %0, %1, dim = 1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"x"}, {}]>]>} : (tensor<4x32x256xf32>, tensor<4x48x256xf32>) -> tensor<4x80x256xf32>
   return %2 : tensor<4x80x256xf32>
 }
diff --git a/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards/dot_dot_general.mlir b/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards/dot_dot_general.mlir
index 990f270..fa41f90 100644
--- a/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards/dot_dot_general.mlir
+++ b/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards/dot_dot_general.mlir
@@ -434,8 +434,8 @@ func.func @dot_result_size_is_equal_to_the_sum_of_operand_sizes(%arg0: tensor<4x
   return %0 : tensor<4x4xf32>
 }
 
-// CHECK-LABEL: func @dot_general_contracting_dim_sharded_multiple_axes
-func.func @dot_general_contracting_dim_sharded_multiple_axes(
+// CHECK-LABEL: func @dot_genaral_contracting_dim_sharded_multiple_axes
+func.func @dot_genaral_contracting_dim_sharded_multiple_axes(
     %arg0: tensor<4x8x32xf32> {sdy.sharding = #sdy.sharding<@mesh_xyz, [{"y"}, {}, {"x", "z"}]>},
     %arg1: tensor<4x32x16xf32> {sdy.sharding = #sdy.sharding<@mesh_xyz, [{"y"}, {"x", "z"}, {}]>})
     -> (tensor<4x8x16xf32> {sdy.sharding = #sdy.sharding<@mesh_xyz, [{"y"}, {}, {}]>}) {
@@ -446,8 +446,8 @@ func.func @dot_general_contracting_dim_sharded_multiple_axes(
   return %0 : tensor<4x8x16xf32>
 }
 
-// CHECK-LABEL: func @dot_general_multiple_contracting_dims_sharded
-func.func @dot_general_multiple_contracting_dims_sharded(
+// CHECK-LABEL: func @dot_genaral_multiple_contracting_dims_sharded
+func.func @dot_genaral_multiple_contracting_dims_sharded(
     %arg0: tensor<4x8x32xf32> {sdy.sharding = #sdy.sharding<@mesh_xyz, [{}, {"y"}, {"x", "z"}]>},
     %arg1: tensor<8x32x16xf32> {sdy.sharding = #sdy.sharding<@mesh_xyz, [{"y"}, {"x", "z"}, {}]>})
     -> (tensor<4x16xf32> {sdy.sharding = #sdy.sharding<@mesh_xyz, [{}, {}]>}) {
@@ -459,8 +459,8 @@ func.func @dot_general_multiple_contracting_dims_sharded(
 }
 
 // The following test target is from b/410643498.
-// CHECK-LABEL: func @dot_general_multiple_contracting_dims_conflicts
-func.func @dot_general_multiple_contracting_dims_conflicts(
+// CHECK-LABEL: func @dot_genaral_multiple_contracting_dims_conflicts
+func.func @dot_genaral_multiple_contracting_dims_conflicts(
   %arg0: tensor<16x32x64xf32> {sdy.sharding = #sdy.sharding<@mesh_xyzt, [{"x"}, {"z"}, {"t"}]>},
   %arg1: tensor<32x64x128xf32> {sdy.sharding = #sdy.sharding<@mesh_xyzt, [{"z"}, {"t"}, {"x", "y"}]>})
   ->(tensor<16x128xf32> {sdy.sharding = #sdy.sharding<@mesh_xyzt, [{"x"}, {"z", "t"}]>}) {
@@ -473,8 +473,8 @@ func.func @dot_general_multiple_contracting_dims_conflicts(
   return %0 : tensor<16x128xf32>
 }
 
-// CHECK-LABEL: func @dot_general_incompatible_with_batching_dims
-func.func @dot_general_incompatible_with_batching_dims(%arg0: tensor<4x8x32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {"y"}, {}]>}, %arg1: tensor<4x32x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}, {"y"}]>}) -> (tensor<4x8x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"x"}, {"y"}]>}) {
+// CHECK-LABEL: func @dot_genaral_incompatable_with_batching_dims
+func.func @dot_genaral_incompatable_with_batching_dims(%arg0: tensor<4x8x32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {"y"}, {}]>}, %arg1: tensor<4x32x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}, {"y"}]>}) -> (tensor<4x8x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"x"}, {"y"}]>}) {
   // CHECK-NEXT: %[[RESHARD1:.*]] = sdy.reshard %arg0 <@mesh, [{"x"}, {}, {}]> : tensor<4x8x32xf32>
   // CHECK-NEXT: %[[DOTGENERAL:.*]] = stablehlo.dot_general %[[RESHARD1]], %arg1, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"x"}, {}, {"y"}]>]>} : (tensor<4x8x32xf32>, tensor<4x32x16xf32>) -> tensor<4x8x16xf32>
   // CHECK-NEXT: %[[RESHARD2:.*]] = sdy.reshard %[[DOTGENERAL]] <@mesh, [{}, {"x"}, {"y"}]> : tensor<4x8x16xf32>
@@ -483,8 +483,8 @@ func.func @dot_general_incompatible_with_batching_dims(%arg0: tensor<4x8x32xf32>
   return %0 : tensor<4x8x16xf32>
 }
 
-// CHECK-LABEL: func @dot_general_incompatible_batching_factor_mismatch_on_all_tensors
-func.func @dot_general_incompatible_batching_factor_mismatch_on_all_tensors(%arg0: tensor<4x8x32xf32> {sdy.sharding = #sdy.sharding<@mesh_xyz, [{"x"}, {}, {}]>}, %arg1: tensor<4x32x16xf32> {sdy.sharding = #sdy.sharding<@mesh_xyz, [{"y"}, {}, {}]>}) -> (tensor<4x8x16xf32> {sdy.sharding = #sdy.sharding<@mesh_xyz, [{"z"}, {}, {}]>}) {
+// CHECK-LABEL: func @dot_genaral_incompatable_batching_factor_mismatch_on_all_tensors
+func.func @dot_genaral_incompatable_batching_factor_mismatch_on_all_tensors(%arg0: tensor<4x8x32xf32> {sdy.sharding = #sdy.sharding<@mesh_xyz, [{"x"}, {}, {}]>}, %arg1: tensor<4x32x16xf32> {sdy.sharding = #sdy.sharding<@mesh_xyz, [{"y"}, {}, {}]>}) -> (tensor<4x8x16xf32> {sdy.sharding = #sdy.sharding<@mesh_xyz, [{"z"}, {}, {}]>}) {
   // CHECK-NEXT: %[[RESHARD1:.*]] = sdy.reshard %arg0 <@mesh_xyz, [{"y"}, {}, {}]> : tensor<4x8x32xf32>
   // CHECK-NEXT: %[[DOTGENERAL:.*]] = stablehlo.dot_general %[[RESHARD1]], %arg1, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh_xyz, [{"y"}, {}, {}]>]>} : (tensor<4x8x32xf32>, tensor<4x32x16xf32>) -> tensor<4x8x16xf32>
   // CHECK-NEXT: %[[RESHARD2:.*]] = sdy.reshard %[[DOTGENERAL]] <@mesh_xyz, [{"z"}, {}, {}]> : tensor<4x8x16xf32>
@@ -493,8 +493,8 @@ func.func @dot_general_incompatible_batching_factor_mismatch_on_all_tensors(%arg
   return %0 : tensor<4x8x16xf32>
 }
 
-// CHECK-LABEL: func @dot_general_one_is_strict_prefix_of_other
-func.func @dot_general_one_is_strict_prefix_of_other(%arg0: tensor<4x8x32xf32> {sdy.sharding = #sdy.sharding<@mesh_xyz, [{"y", "x":(1)2}, {}, {}]>}, %arg1: tensor<4x32x16xf32> {sdy.sharding = #sdy.sharding<@mesh_xyz, [{"y"}, {}, {}]>}) -> tensor<4x8x16xf32> {
+// CHECK-LABEL: func @dot_genaral_one_is_strict_prefix_of_other
+func.func @dot_genaral_one_is_strict_prefix_of_other(%arg0: tensor<4x8x32xf32> {sdy.sharding = #sdy.sharding<@mesh_xyz, [{"y", "x":(1)2}, {}, {}]>}, %arg1: tensor<4x32x16xf32> {sdy.sharding = #sdy.sharding<@mesh_xyz, [{"y"}, {}, {}]>}) -> tensor<4x8x16xf32> {
   // CHECK-NEXT: %[[RESHARD1:.*]] = sdy.reshard %arg1 <@mesh_xyz, [{"y", "x":(1)2}, {}, {}]> : tensor<4x32x16xf32>
   // CHECK-NEXT: %[[DOT:.*]] = stablehlo.dot_general %arg0, %[[RESHARD1]], batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh_xyz, [{"y", "x":(1)2}, {}, {}]>]>} : (tensor<4x8x32xf32>, tensor<4x32x16xf32>) -> tensor<4x8x16xf32>
   // CHECK-NEXT: %[[RESHARD2:.*]] = sdy.reshard %[[DOT]] <@mesh_xyz, [{}, {}, {}]> : tensor<4x8x16xf32>
@@ -503,8 +503,8 @@ func.func @dot_general_one_is_strict_prefix_of_other(%arg0: tensor<4x8x32xf32> {
   return %0 : tensor<4x8x16xf32>
 }
 
-// CHECK-LABEL: func @dot_general_one_prefix_has_larger_count
-func.func @dot_general_one_prefix_has_larger_count(%arg0: tensor<4x8x32xf32> {sdy.sharding = #sdy.sharding<@mesh_xyz, [{"y", "x":(1)2}, {}, {}]>}, %arg1: tensor<4x32x16xf32> {sdy.sharding = #sdy.sharding<@mesh_xyz, [{"y", "x"}, {}, {}]>}) ->(tensor<4x8x16xf32> {sdy.sharding = #sdy.sharding<@mesh_xyz, [{"z"}, {}, {}]>}) {
+// CHECK-LABEL: func @dot_genaral_one_prefix_has_larger_count
+func.func @dot_genaral_one_prefix_has_larger_count(%arg0: tensor<4x8x32xf32> {sdy.sharding = #sdy.sharding<@mesh_xyz, [{"y", "x":(1)2}, {}, {}]>}, %arg1: tensor<4x32x16xf32> {sdy.sharding = #sdy.sharding<@mesh_xyz, [{"y", "x"}, {}, {}]>}) ->(tensor<4x8x16xf32> {sdy.sharding = #sdy.sharding<@mesh_xyz, [{"z"}, {}, {}]>}) {
   // CHECK-NEXT: %[[RESHARD1:.*]] = sdy.reshard %arg0 <@mesh_xyz, [{"y", "x"}, {}, {}]> : tensor<4x8x32xf32>
   // CHECK-NEXT: %[[DOTGENERAL:.*]] = stablehlo.dot_general %[[RESHARD1]], %arg1, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh_xyz, [{"y", "x"}, {}, {}]>]>} : (tensor<4x8x32xf32>, tensor<4x32x16xf32>) -> tensor<4x8x16xf32>
   // CHECK-NEXT: %[[RESHARD2:.*]] = sdy.reshard %[[DOTGENERAL]] <@mesh_xyz, [{"z"}, {}, {}]> : tensor<4x8x16xf32>
@@ -513,8 +513,8 @@ func.func @dot_general_one_prefix_has_larger_count(%arg0: tensor<4x8x32xf32> {sd
   return %0 : tensor<4x8x16xf32>
 }
 
-// CHECK-LABEL: func @dot_general_one_suffix_has_larger_count_on_another_factor
-func.func @dot_general_one_suffix_has_larger_count_on_another_factor(%arg0: tensor<4x8x8xf32> {sdy.sharding = #sdy.sharding<@mesh_xyz, [{"y", "x":(1)2}, {"x":(2)2}, {}]>}, %arg1: tensor<4x8x16xf32> {sdy.sharding = #sdy.sharding<@mesh_xyz, [{"y", "x"}, {}, {}]>}) ->(tensor<4x8x16xf32> {sdy.sharding = #sdy.sharding<@mesh_xyz, [{}, {"x":(2)2}, {}]>}) {
+// CHECK-LABEL: func @dot_genaral_one_suffix_has_larger_count_on_another_factor
+func.func @dot_genaral_one_suffix_has_larger_count_on_another_factor(%arg0: tensor<4x8x8xf32> {sdy.sharding = #sdy.sharding<@mesh_xyz, [{"y", "x":(1)2}, {"x":(2)2}, {}]>}, %arg1: tensor<4x8x16xf32> {sdy.sharding = #sdy.sharding<@mesh_xyz, [{"y", "x"}, {}, {}]>}) ->(tensor<4x8x16xf32> {sdy.sharding = #sdy.sharding<@mesh_xyz, [{}, {"x":(2)2}, {}]>}) {
   // CHECK-NEXT: %[[RESHARD1:.*]] = sdy.reshard %arg1 <@mesh_xyz, [{"y", "x":(1)2}, {}, {}]> : tensor<4x8x16xf32>
   // CHECK-NEXT: %[[DOTGENERAL:.*]] = stablehlo.dot_general %arg0, %[[RESHARD1]], batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh_xyz, [{"y", "x":(1)2}, {"x":(2)2}, {}]>]>} : (tensor<4x8x8xf32>, tensor<4x8x16xf32>) -> tensor<4x8x16xf32>
   // CHECK-NEXT: %[[RESHARD2:.*]] = sdy.reshard %[[DOTGENERAL]] <@mesh_xyz, [{}, {"x":(2)2}, {}]> : tensor<4x8x16xf32>
@@ -524,18 +524,19 @@ func.func @dot_general_one_suffix_has_larger_count_on_another_factor(%arg0: tens
 }
 
 // TODO(zixuanjiang). We may want to keep {"y", "x":(1)2, "t":(2)2} for the batch dimension.
-// CHECK-LABEL: func @dot_general_batching_dimension_shardings_have_common_prefix
-func.func @dot_general_batching_dimension_shardings_have_common_prefix(%arg0: tensor<64x8x32xf32> {sdy.sharding = #sdy.sharding<@mesh_xyzt, [{"y", "x":(1)2, "t":(1)2}, {"t":(2)2}, {}]>}, %arg1: tensor<64x32x16xf32> {sdy.sharding = #sdy.sharding<@mesh_xyzt, [{"y", "x":(1)2, "t":(2)2}, {}, {"t":(1)2}]>}) ->(tensor<64x8x16xf32> {sdy.sharding = #sdy.sharding<@mesh_xyzt, [{}, {"t":(2)2}, {"t":(1)2}]>}) {
-  // CHECK: %[[RESHARD1:.*]] = sdy.reshard %arg0 <@mesh_xyzt, [{"y", "x":(1)2, "t":(2)2}, {}, {}]> : tensor<64x8x32xf32>
-  // CHECK-NEXT: %[[DOTGENERAL:.*]] = stablehlo.dot_general %[[RESHARD1]], %arg1, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh_xyzt, [{"y", "x":(1)2, "t":(2)2}, {}, {"t":(1)2}]>]>} : (tensor<64x8x32xf32>, tensor<64x32x16xf32>) -> tensor<64x8x16xf32>
+// CHECK-LABEL: func @dot_genaral_batching_dimension_shardings_have_common_prefix
+func.func @dot_genaral_batching_dimension_shardings_have_common_prefix(%arg0: tensor<64x8x32xf32> {sdy.sharding = #sdy.sharding<@mesh_xyzt, [{"y", "x":(1)2, "t":(1)2}, {"t":(2)2}, {}]>}, %arg1: tensor<64x32x16xf32> {sdy.sharding = #sdy.sharding<@mesh_xyzt, [{"y", "x":(1)2, "t":(2)2}, {}, {"t":(1)2}]>}) ->(tensor<64x8x16xf32> {sdy.sharding = #sdy.sharding<@mesh_xyzt, [{}, {"t":(2)2}, {"t":(1)2}]>}) {
+  // CHECK: %[[RESHARD1:.*]] = sdy.reshard %arg0 <@mesh_xyzt, [{"y", "x":(1)2}, {"t":(2)2}, {}]> : tensor<64x8x32xf32>
+  // CHECK: %[[RESHARD2:.*]] = sdy.reshard %arg1 <@mesh_xyzt, [{"y", "x":(1)2}, {}, {"t":(1)2}]> : tensor<64x32x16xf32>
+  // CHECK-NEXT: %[[DOTGENERAL:.*]] = stablehlo.dot_general %[[RESHARD1]], %[[RESHARD2]], batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh_xyzt, [{"y", "x":(1)2}, {"t":(2)2}, {"t":(1)2}]>]>} : (tensor<64x8x32xf32>, tensor<64x32x16xf32>) -> tensor<64x8x16xf32>
   // CHECK-NEXT: %[[RESHARD2:.*]] = sdy.reshard %[[DOTGENERAL]] <@mesh_xyzt, [{}, {"t":(2)2}, {"t":(1)2}]> : tensor<64x8x16xf32>
   // CHECK-NEXT: return %[[RESHARD2]] : tensor<64x8x16xf32>
   %0 = stablehlo.dot_general %arg0, %arg1, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh_xyzt, [{}, {"t":(2)2}, {"t":(1)2}]>]>} : (tensor<64x8x32xf32>, tensor<64x32x16xf32>) -> tensor<64x8x16xf32>
   return %0 : tensor<64x8x16xf32>
 }
 
-// CHECK-LABEL: func @dot_general_overlaps_and_trimmable
-func.func @dot_general_overlaps_and_trimmable(%arg0: tensor<64x8x32xf32> {sdy.sharding = #sdy.sharding<@mesh_xyzt, [{"y":(1)2}, {}, {}]>}, %arg1: tensor<64x32x16xf32> {sdy.sharding = #sdy.sharding<@mesh_xyzt, [{"y":(1)2}, {}, {}]>}) ->(tensor<64x8x16xf32> {sdy.sharding = #sdy.sharding<@mesh_xyzt, [{}, {"x","y","z"}, {}]>}) {
+// CHECK-LABEL: func @dot_genaral_overlaps_and_trimmable
+func.func @dot_genaral_overlaps_and_trimmable(%arg0: tensor<64x8x32xf32> {sdy.sharding = #sdy.sharding<@mesh_xyzt, [{"y":(1)2}, {}, {}]>}, %arg1: tensor<64x32x16xf32> {sdy.sharding = #sdy.sharding<@mesh_xyzt, [{"y":(1)2}, {}, {}]>}) ->(tensor<64x8x16xf32> {sdy.sharding = #sdy.sharding<@mesh_xyzt, [{}, {"x","y","z"}, {}]>}) {
   // CHECK-NEXT: %[[RESHARD1:.*]] = sdy.reshard %arg0 <@mesh_xyzt, [{"y":(1)2}, {"x"}, {}]> : tensor<64x8x32xf32>
   // CHECK-NEXT: %[[DOTGENERAL:.*]] = stablehlo.dot_general %0, %arg1, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh_xyzt, [{"y":(1)2}, {"x"}, {}]>]>} : (tensor<64x8x32xf32>, tensor<64x32x16xf32>) -> tensor<64x8x16xf32>
   // CHECK-NEXT: %[[RESHARD2:.*]] = sdy.reshard %1 <@mesh_xyzt, [{}, {"x", "y", "z"}, {}]> : tensor<64x8x16xf32>
@@ -544,8 +545,8 @@ func.func @dot_general_overlaps_and_trimmable(%arg0: tensor<64x8x32xf32> {sdy.sh
   return %0 : tensor<64x8x16xf32>
 }
 
-// CHECK-LABEL: func @dot_general_overlaps_from_most_major
-func.func @dot_general_overlaps_from_most_major(%arg0: tensor<64x8x32xf32> {sdy.sharding = #sdy.sharding<@mesh_xyzt, [{"y":(1)2}, {}, {}]>}, %arg1: tensor<64x32x16xf32> {sdy.sharding = #sdy.sharding<@mesh_xyzt, [{"y":(1)2}, {}, {}]>}) ->(tensor<64x8x16xf32> {sdy.sharding = #sdy.sharding<@mesh_xyzt, [{}, {"y"}, {}]>}) {
+// CHECK-LABEL: func @dot_genaral_overlaps_from_most_major
+func.func @dot_genaral_overlaps_from_most_major(%arg0: tensor<64x8x32xf32> {sdy.sharding = #sdy.sharding<@mesh_xyzt, [{"y":(1)2}, {}, {}]>}, %arg1: tensor<64x32x16xf32> {sdy.sharding = #sdy.sharding<@mesh_xyzt, [{"y":(1)2}, {}, {}]>}) ->(tensor<64x8x16xf32> {sdy.sharding = #sdy.sharding<@mesh_xyzt, [{}, {"y"}, {}]>}) {
   // CHECK-NEXT: %[[DOTGENERAL:.*]] = stablehlo.dot_general %arg0, %arg1, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh_xyzt, [{"y":(1)2}, {}, {}]>]>} : (tensor<64x8x32xf32>, tensor<64x32x16xf32>) -> tensor<64x8x16xf32>
   // CHECK-NEXT: %[[RESHARD:.*]] = sdy.reshard %[[DOTGENERAL]] <@mesh_xyzt, [{}, {"y"}, {}]> : tensor<64x8x16xf32>
   // CHECK-NEXT: return %[[RESHARD]] : tensor<64x8x16xf32>
@@ -553,8 +554,8 @@ func.func @dot_general_overlaps_from_most_major(%arg0: tensor<64x8x32xf32> {sdy.
   return %0 : tensor<64x8x16xf32>
 }
 
-// CHECK-LABEL: func @dot_general_overlaps_and_trimmable_on_subaxis
-func.func @dot_general_overlaps_and_trimmable_on_subaxis(%arg0: tensor<64x8x32xf32> {sdy.sharding = #sdy.sharding<@mesh_xyzt, [{"y":(2)2}, {}, {}]>}, %arg1: tensor<64x32x16xf32> {sdy.sharding = #sdy.sharding<@mesh_xyzt, [{"y":(2)2}, {}, {}]>}) ->(tensor<64x8x16xf32> {sdy.sharding = #sdy.sharding<@mesh_xyzt, [{}, {"y"}, {}]>}) {
+// CHECK-LABEL: func @dot_genaral_overlaps_and_trimmable_on_subaxis
+func.func @dot_genaral_overlaps_and_trimmable_on_subaxis(%arg0: tensor<64x8x32xf32> {sdy.sharding = #sdy.sharding<@mesh_xyzt, [{"y":(2)2}, {}, {}]>}, %arg1: tensor<64x32x16xf32> {sdy.sharding = #sdy.sharding<@mesh_xyzt, [{"y":(2)2}, {}, {}]>}) ->(tensor<64x8x16xf32> {sdy.sharding = #sdy.sharding<@mesh_xyzt, [{}, {"y"}, {}]>}) {
   // CHECK-NEXT: %[[RESHARD1:.*]] = sdy.reshard %arg0 <@mesh_xyzt, [{"y":(2)2}, {"y":(1)2}, {}]> : tensor<64x8x32xf32>
   // CHECK-NEXT: %[[DOTGENERAL:.*]] = stablehlo.dot_general %[[RESHARD1]], %arg1, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh_xyzt, [{"y":(2)2}, {"y":(1)2}, {}]>]>} : (tensor<64x8x32xf32>, tensor<64x32x16xf32>) -> tensor<64x8x16xf32>
   // CHECK-NEXT: %[[RESHARD2:.*]] = sdy.reshard %[[DOTGENERAL]] <@mesh_xyzt, [{}, {"y"}, {}]> : tensor<64x8x16xf32>
@@ -563,8 +564,8 @@ func.func @dot_general_overlaps_and_trimmable_on_subaxis(%arg0: tensor<64x8x32xf
   return %0 : tensor<64x8x16xf32>
 }
 
-// CHECK-LABEL: func @dot_general_overlaps_and_trimmable_on_subaxis_multiple_axes
-func.func @dot_general_overlaps_and_trimmable_on_subaxis_multiple_axes(%arg0: tensor<64x8x32xf32> {sdy.sharding = #sdy.sharding<@mesh_xyzt, [{"y":(2)2}, {}, {}]>}, %arg1: tensor<64x32x16xf32> {sdy.sharding = #sdy.sharding<@mesh_xyzt, [{"y":(2)2}, {}, {}]>}) ->(tensor<64x8x16xf32> {sdy.sharding = #sdy.sharding<@mesh_xyzt, [{}, {"x","y","z"}, {}]>}) {
+// CHECK-LABEL: func @dot_genaral_overlaps_and_trimmable_on_subaxis_multiple_axes
+func.func @dot_genaral_overlaps_and_trimmable_on_subaxis_multiple_axes(%arg0: tensor<64x8x32xf32> {sdy.sharding = #sdy.sharding<@mesh_xyzt, [{"y":(2)2}, {}, {}]>}, %arg1: tensor<64x32x16xf32> {sdy.sharding = #sdy.sharding<@mesh_xyzt, [{"y":(2)2}, {}, {}]>}) ->(tensor<64x8x16xf32> {sdy.sharding = #sdy.sharding<@mesh_xyzt, [{}, {"x","y","z"}, {}]>}) {
   // CHECK-NEXT: %[[RESHARD1:.*]] = sdy.reshard %arg0 <@mesh_xyzt, [{"y":(2)2}, {"x", "y":(1)2}, {}]> : tensor<64x8x32xf32>
   // CHECK-NEXT: %[[DOTGENERAL:.*]] = stablehlo.dot_general %[[RESHARD1]], %arg1, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh_xyzt, [{"y":(2)2}, {"x", "y":(1)2}, {}]>]>} : (tensor<64x8x32xf32>, tensor<64x32x16xf32>) -> tensor<64x8x16xf32>
   // CHECK-NEXT: %[[RESHARD2:.*]] = sdy.reshard %[[DOTGENERAL]] <@mesh_xyzt, [{}, {"x", "y", "z"}, {}]> : tensor<64x8x16xf32>
@@ -585,11 +586,11 @@ func.func @dot_only_contracting_dims_sharded_and_has_same_shardings(
   return %0 : tensor<8x16xf32>
 }
 
-// The following 4 test targets are analyzed quantitatively in b/448376870#comment6.
+// The following 4 test targets are analyzed quantitlively in b/448376870#comment6.
 // In short, keep the largest factor sharded.
 
-// CHECK-LABEL: func @dot_ij_jk_ik_i_is_largest
-func.func @dot_ij_jk_ik_i_is_largest(
+// CHECK-LABEL: func @dot_ij_jk_ik_i_is_largset
+func.func @dot_ij_jk_ik_i_is_largset(
     %arg0: tensor<16x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"x"}]>},
     %arg1: tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"x"}]>})
     -> (tensor<16x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}]>}) {
@@ -601,8 +602,8 @@ func.func @dot_ij_jk_ik_i_is_largest(
   return %0 : tensor<16x8xf32>
 }
 
-// CHECK-LABEL: func @dot_ij_jk_ik_j_is_largest
-func.func @dot_ij_jk_ik_j_is_largest(
+// CHECK-LABEL: func @dot_ij_jk_ik_j_is_largset
+func.func @dot_ij_jk_ik_j_is_largset(
     %arg0: tensor<8x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"x"}]>},
     %arg1: tensor<16x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"x"}]>})
     -> (tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}]>}) {
@@ -615,8 +616,8 @@ func.func @dot_ij_jk_ik_j_is_largest(
   return %0 : tensor<8x8xf32>
 }
 
-// CHECK-LABEL: func @dot_ij_jk_ik_k_is_largest
-func.func @dot_ij_jk_ik_k_is_largest(
+// CHECK-LABEL: func @dot_ij_jk_ik_k_is_largset
+func.func @dot_ij_jk_ik_k_is_largset(
     %arg0: tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"x"}]>},
     %arg1: tensor<8x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"x"}]>})
     -> (tensor<8x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}]>}) {
@@ -707,35 +708,29 @@ func.func @dot_on_rectangular_inputs_square_output_large_contracting_dim_lhs_2nd
   return %0 : tensor<8x8xf32>
 }
 
-// TODO(zixuanjiang): Fix the issue that caused this test to change the
-// behaviour from all reduce, reshard of the result of the dot operation to 2
-// input reshards instead.
 // CHECK-LABEL: func @dot_result_is_smaller_than_rhs_due_to_other_axes
 func.func @dot_result_is_smaller_than_rhs_due_to_other_axes(
     %arg0: tensor<8x32x64xf32> {sdy.sharding = #sdy.sharding<@mesh_xyz, [{"x"}, {"z"}, {"y"}]>},
     %arg1: tensor<64x256xf32> {sdy.sharding = #sdy.sharding<@mesh_xyz, [{"y"}, {}]>})
     -> (tensor<8x32x256xf32> {sdy.sharding = #sdy.sharding<@mesh_xyz, [{"x", "y"}, {"z"}, {}]>}) {
-  // CHECK-NEXT: %[[RESHARD1:.*]] = sdy.reshard %arg0 <@mesh_xyz, [{"x", "y"}, {"z"}, {}]> : tensor<8x32x64xf32>
-  // CHECK-NEXT: %[[RESHARD2:.*]] = sdy.reshard %arg1 <@mesh_xyz, [{}, {}]> : tensor<64x256xf32>
-  // CHECK-NEXT: %[[DOT:.*]] = stablehlo.dot_general %[[RESHARD1]], %[[RESHARD2]], contracting_dims = [2] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh_xyz, [{"x", "y"}, {"z"}, {}]>]>} : (tensor<8x32x64xf32>, tensor<64x256xf32>) -> tensor<8x32x256xf32>
-  // CHECK-NEXT: return %[[DOT]]
+  // CHECK-NEXT: %[[DOT:.*]] = stablehlo.dot_general %arg0, %arg1
+  // CHECK-SAME: {sdy.sharding = #sdy.sharding_per_value<[<@mesh_xyz, [{"x"}, {"z"}, {}]>]>}
+  // CHECK-NEXT: %[[ALL_REDUCE:.*]] = sdy.all_reduce {"y"} %[[DOT]] out_sharding=<@mesh_xyz, [{"x"}, {"z"}, {}]>
+  // CHECK-NEXT: %[[RESHARD:.*]] = sdy.reshard %[[ALL_REDUCE]] <@mesh_xyz, [{"x", "y"}, {"z"}, {}]>
+  // CHECK-NEXT: return %[[RESHARD]]
   %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [2] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh_xyz, [{"x", "y"}, {"z"}, {}]>]>} : (tensor<8x32x64xf32>, tensor<64x256xf32>) -> tensor<8x32x256xf32>
   return %0 : tensor<8x32x256xf32>
 }
 
-// TODO(b/448376870): Fix the issue that caused this test to change the
-// behaviour from input reshards to one input reshard and all reduce, reshard of
-// the result of the dot operation.
 // CHECK-LABEL: func @dot_all_factors_have_the_same_sharding_one_non_contracting_dim_is_largest_the_other_smallest_result_tensor_is_sharded_on_larger_factor
 func.func @dot_all_factors_have_the_same_sharding_one_non_contracting_dim_is_largest_the_other_smallest_result_tensor_is_sharded_on_larger_factor(
     %arg0: tensor<128x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"x"}]>},
     %arg1: tensor<8x4xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"x"}]>})
     -> (tensor<128x4xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}]>}) {
-  // CHECK-NEXT: %[[RESHARD1:.*]] = sdy.reshard %arg1 <@mesh, [{"x"}, {}]> : tensor<8x4xf32>
-  // CHECK-NEXT: %[[DOT:.*]] = stablehlo.dot %arg0, %[[RESHARD1]] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}]>]>} : (tensor<128x8xf32>, tensor<8x4xf32>) -> tensor<128x4xf32>
-  // CHECK-NEXT: %[[ALL_REDUCE:.*]] = sdy.all_reduce {"x"} %1 out_sharding=<@mesh, [{}, {}]>
-  // CHECK-NEXT: %[[RESHARD2:.*]] = sdy.reshard %[[ALL_REDUCE]] <@mesh, [{"x"}, {}]>
-  // CHECK-NEXT: return %[[RESHARD2]]
+  // CHECK-NEXT: %[[RESHARD0:.*]] = sdy.reshard %arg0 <@mesh, [{"x"}, {}]>
+  // CHECK-NEXT: %[[RESHARD1:.*]] = sdy.reshard %arg1 <@mesh, [{}, {}]>
+  // CHECK-NEXT: %[[DOT:.*]] = stablehlo.dot %[[RESHARD0]], %[[RESHARD1]] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"x"}, {}]>]>}
+  // CHECK-NEXT: return %[[DOT]]
   %0 = stablehlo.dot %arg0, %arg1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"x"}, {}]>]>} : (tensor<128x8xf32>, tensor<8x4xf32>) -> tensor<128x4xf32>
   return %0 : tensor<128x4xf32>
 }
diff --git a/third_party/llvm/generated.patch b/third_party/llvm/generated.patch
index a6f7fe5..1443533 100644
--- a/third_party/llvm/generated.patch
+++ b/third_party/llvm/generated.patch
@@ -1,89 +1,38 @@
 Auto generated patch. Do not edit or delete it, even if empty.
-diff -ruN --strip-trailing-cr a/llvm/lib/Target/BPF/BPFAsmPrinter.cpp b/llvm/lib/Target/BPF/BPFAsmPrinter.cpp
---- a/llvm/lib/Target/BPF/BPFAsmPrinter.cpp
-+++ b/llvm/lib/Target/BPF/BPFAsmPrinter.cpp
-@@ -176,10 +176,6 @@
-         if (const GlobalValue *GV = Op.getGlobal())
-           if (GV->getName() == BPF_TRAP)
-             SawTrapCall = true;
--      } else if (Op.isSymbol()) {
--        if (const MCSymbol *Sym = Op.getMCSymbol())
--          if (Sym->getName() == BPF_TRAP)
--            SawTrapCall = true;
+diff -ruN --strip-trailing-cr a/libc/src/stdio/printf_core/vfprintf_internal.h b/libc/src/stdio/printf_core/vfprintf_internal.h
+--- a/libc/src/stdio/printf_core/vfprintf_internal.h
++++ b/libc/src/stdio/printf_core/vfprintf_internal.h
+@@ -51,8 +51,11 @@
+ LIBC_INLINE FileIOResult fwrite_unlocked(const void *ptr, size_t size,
+                                          size_t nmemb, ::FILE *f) {
+   // Need to use system errno in this case, as system write will set this errno
+-  // which we need to propagate back into our code.
+-  return {::fwrite_unlocked(ptr, size, nmemb, f), errno};
++  // which we need to propagate back into our code. fwrite only modifies errno
++  // if there was an error, and errno may have previously been nonzero. Only
++  // return errno if there was an error.
++  size_t members_written = ::fwrite_unlocked(ptr, size, nmemb, f);
++  return {members_written, members_written == nmemb ? 0 : errno};
+ }
+ #endif // LIBC_COPT_STDIO_USE_SYSTEM_FILE
+ } // namespace internal
+diff -ruN --strip-trailing-cr a/libcxx/include/fstream b/libcxx/include/fstream
+--- a/libcxx/include/fstream
++++ b/libcxx/include/fstream
+@@ -315,8 +315,14 @@
+         traits_type::copy(__str, this->gptr(), __n);
+         this->__gbump_ptrdiff(__n);
        }
-     }
-   }
-diff -ruN --strip-trailing-cr a/llvm/test/Analysis/CostModel/AArch64/sincos.ll b/llvm/test/Analysis/CostModel/AArch64/sincos.ll
---- a/llvm/test/Analysis/CostModel/AArch64/sincos.ll
-+++ b/llvm/test/Analysis/CostModel/AArch64/sincos.ll
-@@ -38,14 +38,14 @@
- ;
- ; SINCOS_STRET-LABEL: 'sincos'
- ; SINCOS_STRET:  Cost Model: Found an estimated cost of 1 for instruction: %f16 = call { half, half } @llvm.sincos.f16(half poison)
--; SINCOS_STRET:  Cost Model: Found an estimated cost of 2 for instruction: %f32 = call { float, float } @llvm.sincos.f32(float poison)
--; SINCOS_STRET:  Cost Model: Found an estimated cost of 2 for instruction: %f64 = call { double, double } @llvm.sincos.f64(double poison)
-+; SINCOS_STRET:  Cost Model: Found an estimated cost of 10 for instruction: %f32 = call { float, float } @llvm.sincos.f32(float poison)
-+; SINCOS_STRET:  Cost Model: Found an estimated cost of 10 for instruction: %f64 = call { double, double } @llvm.sincos.f64(double poison)
- ; SINCOS_STRET:  Cost Model: Found an estimated cost of 10 for instruction: %f128 = call { fp128, fp128 } @llvm.sincos.f128(fp128 poison)
- ; SINCOS_STRET:  Cost Model: Found an estimated cost of 36 for instruction: %v8f16 = call { <8 x half>, <8 x half> } @llvm.sincos.v8f16(<8 x half> poison)
--; SINCOS_STRET:  Cost Model: Found an estimated cost of 20 for instruction: %v4f32 = call { <4 x float>, <4 x float> } @llvm.sincos.v4f32(<4 x float> poison)
--; SINCOS_STRET:  Cost Model: Found an estimated cost of 8 for instruction: %v2f64 = call { <2 x double>, <2 x double> } @llvm.sincos.v2f64(<2 x double> poison)
-+; SINCOS_STRET:  Cost Model: Found an estimated cost of 52 for instruction: %v4f32 = call { <4 x float>, <4 x float> } @llvm.sincos.v4f32(<4 x float> poison)
-+; SINCOS_STRET:  Cost Model: Found an estimated cost of 24 for instruction: %v2f64 = call { <2 x double>, <2 x double> } @llvm.sincos.v2f64(<2 x double> poison)
- ; SINCOS_STRET:  Cost Model: Found an estimated cost of 10 for instruction: %v1f128 = call { <1 x fp128>, <1 x fp128> } @llvm.sincos.v1f128(<1 x fp128> poison)
--; SINCOS_STRET:  Cost Model: Found an estimated cost of 40 for instruction: %v8f32 = call { <8 x float>, <8 x float> } @llvm.sincos.v8f32(<8 x float> poison)
-+; SINCOS_STRET:  Cost Model: Found an estimated cost of 104 for instruction: %v8f32 = call { <8 x float>, <8 x float> } @llvm.sincos.v8f32(<8 x float> poison)
- ; SINCOS_STRET:  Cost Model: Invalid cost for instruction: %nxv8f16 = call { <vscale x 8 x half>, <vscale x 8 x half> } @llvm.sincos.nxv8f16(<vscale x 8 x half> poison)
- ; SINCOS_STRET:  Cost Model: Invalid cost for instruction: %nxv4f32 = call { <vscale x 4 x float>, <vscale x 4 x float> } @llvm.sincos.nxv4f32(<vscale x 4 x float> poison)
- ; SINCOS_STRET:  Cost Model: Invalid cost for instruction: %nxv2f64 = call { <vscale x 2 x double>, <vscale x 2 x double> } @llvm.sincos.nxv2f64(<vscale x 2 x double> poison)
-diff -ruN --strip-trailing-cr a/mlir/lib/Dialect/MemRef/IR/MemRefOps.cpp b/mlir/lib/Dialect/MemRef/IR/MemRefOps.cpp
---- a/mlir/lib/Dialect/MemRef/IR/MemRefOps.cpp
-+++ b/mlir/lib/Dialect/MemRef/IR/MemRefOps.cpp
-@@ -2568,11 +2568,6 @@
-     auto trailingReassocs = ArrayRef<int64_t>(reassoc).drop_front();
-     auto stride = SaturatedInteger::wrap(resultStrides[resultStrideIndex--]);
-     for (int64_t idx : llvm::reverse(trailingReassocs)) {
--      // Dimensions of size 1 should be skipped, because their strides are
--      // meaningless and could have any arbitrary value.
--      if (srcShape[idx - 1] == 1)
--        continue;
--
-       stride = stride * SaturatedInteger::wrap(srcShape[idx]);
- 
-       // Both source and result stride must have the same static value. In that
-@@ -2587,6 +2582,11 @@
-       if (strict && (stride.saturated || srcStride.saturated))
-         return failure();
- 
-+      // Dimensions of size 1 should be skipped, because their strides are
-+      // meaningless and could have any arbitrary value.
-+      if (srcShape[idx - 1] == 1)
-+        continue;
+-      if (__len - __n >= this->egptr() - this->eback())
+-        return std::fread(__str + __n, sizeof(char_type), __len - __n, __file_);
++      const streamsize __remainder    = __len - __n;
++      const streamsize __buffer_space = this->egptr() - this->eback();
 +
-       if (!stride.saturated && !srcStride.saturated && stride != srcStride)
-         return failure();
++      if (__remainder >= __buffer_space)
++        return std::fread(__str + __n, sizeof(char_type), __remainder, __file_) + __n;
++      else if (__remainder > 0)
++        return basic_streambuf<_CharT, _Traits>::xsgetn(__str + __n, __remainder) + __n;
++      return __n;
      }
-diff -ruN --strip-trailing-cr a/mlir/test/Dialect/MemRef/ops.mlir b/mlir/test/Dialect/MemRef/ops.mlir
---- a/mlir/test/Dialect/MemRef/ops.mlir
-+++ b/mlir/test/Dialect/MemRef/ops.mlir
-@@ -440,8 +440,7 @@
-          %arg4: index,
-          %arg5: index,
-          %arg6: index,
--         %arg7: memref<4x?x4xf32>,
--         %arg8: memref<1x1x18x?xsi8, strided<[?, ?, ?, 1], offset: ?>>) {
-+         %arg7: memref<4x?x4xf32>) {
- //       CHECK:   memref.collapse_shape {{.*}} {{\[}}[0, 1], [2]]
- //  CHECK-SAME:     memref<?x?x?xf32> into memref<?x?xf32>
-   %0 = memref.collapse_shape %arg0 [[0, 1], [2]] :
-@@ -490,10 +489,6 @@
- //       CHECK:   memref.expand_shape {{.*}} {{\[}}[0, 1], [2], [3, 4]]
-   %4 = memref.expand_shape %arg7 [[0, 1], [2], [3, 4]] output_shape [2, 2, %arg4, 2, 2]
-         : memref<4x?x4xf32> into memref<2x2x?x2x2xf32>
--
--//       CHECK:   memref.collapse_shape {{.*}} {{\[}}[0, 1], [2], [3]]
--//  CHECK-SAME:     memref<1x1x18x?xsi8, strided<[?, ?, ?, 1], offset: ?>> into memref<1x18x?xsi8, strided<[?, ?, 1], offset: ?>>
--  %5 = memref.collapse_shape %arg8 [[0, 1], [2], [3]] : memref<1x1x18x?xsi8, strided<[?, ?, ?, 1], offset: ?>> into memref<1x18x?xsi8, strided<[?, ?, 1], offset: ?>>
-   return
- }
- 
+     return basic_streambuf<_CharT, _Traits>::xsgetn(__str, __len);
+   }
diff --git a/third_party/llvm/workspace.bzl b/third_party/llvm/workspace.bzl
index abdee76..0fec012 100644
--- a/third_party/llvm/workspace.bzl
+++ b/third_party/llvm/workspace.bzl
@@ -4,8 +4,8 @@ load("//third_party:repo.bzl", "tf_http_archive")
 
 def repo(name):
     """Imports LLVM."""
-    LLVM_COMMIT = "2bc22ea02edda5926f3e53f141def9bf212ac1db"
-    LLVM_SHA256 = "4a034eda852b3c2d448d38e8661cbac45ae2233a29defeb55913fa5205cd29f7"
+    LLVM_COMMIT = "9625cf6cc0e3e530ea0bed971d85b363f77c49d8"
+    LLVM_SHA256 = "48a0479efdde83a02ece6a168094aa14e6f819c829eb61a133ec124089643356"
 
     tf_http_archive(
         name = name,
diff --git a/third_party/stablehlo/temporary.patch b/third_party/stablehlo/temporary.patch
index 9cb6d61..419fd26 100755
--- a/third_party/stablehlo/temporary.patch
+++ b/third_party/stablehlo/temporary.patch
@@ -101,17 +101,6 @@ diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilderTest.c
  #include "mlir/IR/BuiltinAttributes.h"
  #include "mlir/IR/BuiltinOps.h"
  #include "mlir/IR/DialectRegistry.h"
-diff --ruN a/stablehlo/stablehlo/integrations/python/mlir/dialects/InterpreterOps.td b/stablehlo/stablehlo/integrations/python/mlir/dialects/InterpreterOps.td
---- stablehlo/stablehlo/integrations/python/mlir/dialects/InterpreterOps.td
-+++ stablehlo/stablehlo/integrations/python/mlir/dialects/InterpreterOps.td
-@@ -17,6 +17,6 @@
- #ifndef STABLEHLO_INTEGRATIONS_PYTHON_INTERPRETER_OPS
- #define STABLEHLO_INTEGRATIONS_PYTHON_INTERPRETER_OPS
- 
--include "third_party/stablehlo/stablehlo/reference/InterpreterOps.h"
-+include "stablehlo/reference/InterpreterOps.h"
- 
- #endif
 diff --ruN a/stablehlo/stablehlo/tests/BUILD.bazel b/stablehlo/stablehlo/tests/BUILD.bazel
 --- stablehlo/stablehlo/tests/BUILD.bazel
 +++ stablehlo/stablehlo/tests/BUILD.bazel
@@ -135,20 +124,13 @@ diff --ruN a/stablehlo/stablehlo/tests/TestUtils.cpp b/stablehlo/stablehlo/tests
  #include "llvm/Support/Casting.h"
  #include "mlir/Dialect/Func/IR/FuncOps.h"
  #include "mlir/Dialect/Shape/IR/Shape.h"
-@@ -28,6 +29,7 @@
- #include "mlir/IR/Operation.h"
- #include "mlir/IR/OperationSupport.h"
- #include "mlir/IR/PatternMatch.h"
-+#include "mlir/IR/TypeRange.h"
- #include "mlir/Interfaces/InferTypeOpInterface.h"
- #include "mlir/Interfaces/SideEffectInterfaces.h"
- #include "mlir/Pass/Pass.h"
-@@ -35,11 +37,34 @@
+@@ -35,11 +36,35 @@
  #include "mlir/Support/LLVM.h"
  #include "mlir/Support/LogicalResult.h"
  #include "mlir/Transforms/GreedyPatternRewriteDriver.h"
 +#include "stablehlo/dialect/StablehloOps.h"
 +#include "stablehlo/transforms/StablehloBroadcastLowering.h"
++#include "third_party/llvm/llvm-project/mlir/include/mlir/IR/TypeRange.h"
  
  namespace mlir {
  namespace hlo {
@@ -602,51 +584,6 @@ diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.ml
 +  return %0, %1, %2 : tensor<1xf32>, tensor<2x2xi32>, tensor<3x2xcomplex<f32>>
  }
  
- // -----
-diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir
---- stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir
-+++ stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir
-@@ -27,6 +27,14 @@
-   // CHECK-NOT: stablehlo.constant
-   // CHECK: return %arg0
-   return %1 : tensor<f32>
-+}
-+
-+// CHECK-LABEL: @add_cst_on_rhs_with_attrs
-+func.func @add_cst_on_rhs_with_attrs(%arg0: tensor<f32>) -> tensor<f32> {
-+  %cst = stablehlo.constant dense<1.0> : tensor<f32>
-+  // CHECK: stablehlo.add %arg0, %cst {mhlo.frontend_attributes = {foo = "1"}} : tensor<f32>
-+  %0 = stablehlo.add %cst, %arg0 {mhlo.frontend_attributes = {foo = "1"}} : tensor<f32>
-+  return %0 : tensor<f32>
- }
- 
- // -----
-@@ -976,6 +984,26 @@
-   // CHECK-NOT: stablehlo.constant
-   // CHECK: return %arg0 : tensor<f32>
-   return %0 : tensor<f32>
-+}
-+
-+// CHECK-LABEL: @multiply_by_one_merge_attrs
-+func.func @multiply_by_one_merge_attrs(%arg0: tensor<f32>) -> tensor<f32> {
-+  %cst = stablehlo.constant dense<1.0> : tensor<f32>
-+  %0 = stablehlo.add %arg0, %arg0 {mhlo.frontend_attributes = {bar = "1"}} : tensor<f32>
-+  %1 = stablehlo.multiply %0, %cst {mhlo.frontend_attributes = {foo = "1"}} : tensor<f32>
-+  // CHECK: %[[ADD:.*]] = stablehlo.add %arg0, %arg0 {mhlo.frontend_attributes = {bar = "1", foo = "1"}} : tensor<f32>
-+  // CHECK: return %[[ADD]] : tensor<f32>
-+  return %1 : tensor<f32>
-+}
-+
-+// CHECK-LABEL: @multiply_by_one_merge_attrs_conflict
-+func.func @multiply_by_one_merge_attrs_conflict(%arg0: tensor<f32>) -> tensor<f32> {
-+  %cst = stablehlo.constant dense<1.0> : tensor<f32>
-+  %0 = stablehlo.add %arg0, %arg0 {mhlo.frontend_attributes = {bar = "1", foo = "0"}} : tensor<f32>
-+  %1 = stablehlo.multiply %0, %cst {mhlo.frontend_attributes = {foo = "1"}} : tensor<f32>
-+  // CHECK: %[[ADD:.*]] = stablehlo.add %arg0, %arg0 {mhlo.frontend_attributes = {bar = "1", foo = "1"}} : tensor<f32>
-+  // CHECK: return %[[ADD]] : tensor<f32>
-+  return %1 : tensor<f32>
- }
- 
  // -----
 diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_target_independent_optimization.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_target_independent_optimization.mlir
 --- stablehlo/stablehlo/tests/transforms/stablehlo_target_independent_optimization.mlir
@@ -713,8 +650,6 @@ diff --ruN a/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp b/sta
 +#include <utility>
 +
 +#include "llvm/ADT/STLExtras.h"
-+#include "llvm/ADT/Sequence.h"
-+#include "llvm/ADT/SmallVector.h"
 +#include "llvm/Support/Debug.h"
 +#include "llvm/Support/raw_ostream.h"
 +#include "mlir/IR/Builders.h"
@@ -726,6 +661,8 @@ diff --ruN a/stablehlo/stablehlo/transforms/StablehloBroadcastLowering.cpp b/sta
 +#include "mlir/IR/Value.h"
 +#include "mlir/Support/LLVM.h"
 +#include "stablehlo/dialect/StablehloOps.h"
++#include "third_party/llvm/llvm-project/llvm/include/llvm/ADT/Sequence.h"
++#include "third_party/llvm/llvm-project/llvm/include/llvm/ADT/SmallVector.h"
 +
 +#define DEBUG_TYPE "stablehlo-broadcast-lowering"
 +
@@ -1137,190 +1074,4 @@ diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFold
    patterns->add<FoldConvertOpPattern>(context, options, benefit);
    patterns->add<FoldDivOpPattern>(context, options, benefit);
    patterns->add<FoldDynamicSliceOpPattern>(context, options, benefit);
-diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp
---- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp
-+++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp
-@@ -69,6 +69,54 @@
-   });
- }
- 
-+bool mergeDiscardableAttributes(ValueRange sourceValues,
-+                                ValueRange destValues) {
-+  if (sourceValues.size() != destValues.size()) return false;
-+  bool changed = false;
-+  for (auto [source, dest] : llvm::zip(sourceValues, destValues)) {
-+    if (mergeDiscardableAttributes(source, dest)) changed = true;
-+  }
-+  return changed;
-+}
-+
-+bool mergeDiscardableAttributes(Value sourceValue, Value destValue) {
-+  Operation* sourceOp = sourceValue.getDefiningOp();
-+  Operation* destOp = destValue.getDefiningOp();
-+  if (!sourceOp || !destOp) return false;
-+
-+  auto sourceAttrs = sourceOp->getDiscardableAttrDictionary();
-+  if (!sourceAttrs) return true;
-+
-+  auto destAttrs = destOp->getDiscardableAttrDictionary();
-+  if (!destAttrs) {
-+    destOp->setDiscardableAttrs(sourceAttrs);
-+    return true;
-+  }
-+
-+  NamedAttrList mergedAttrs(destAttrs);
-+  for (auto attr : sourceAttrs.getValue()) {
-+    if (attr.getName() == "mhlo.frontend_attributes" &&
-+        mergedAttrs.get("mhlo.frontend_attributes")) {
-+      // Merge frontend attributes, prioritizing source attributes.
-+      auto destFrontendAttrs =
-+          cast<DictionaryAttr>(mergedAttrs.get("mhlo.frontend_attributes"));
-+      auto sourceFrontendAttrs = cast<DictionaryAttr>(attr.getValue());
-+      NamedAttrList frontendAttrs(destFrontendAttrs);
-+      for (auto sourceAttr : sourceFrontendAttrs) {
-+        frontendAttrs.set(sourceAttr.getName(), sourceAttr.getValue());
-+      }
-+      mergedAttrs.set("mhlo.frontend_attributes",
-+                      frontendAttrs.getDictionary(destOp->getContext()));
-+    } else {
-+      // Otherwise prioritize source attributes
-+      mergedAttrs.set(attr.getName(), attr.getValue());
-+    }
-+  }
-+
-+  destOp->setDiscardableAttrs(mergedAttrs.getDictionary(destOp->getContext()));
-+  return true;
-+}
-+
- template <typename OpType>
- struct SimplifyOpRewritePattern : OpRewritePattern<OpType> {
-   SimplifyOpRewritePattern(
-diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td
---- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td
-+++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td
-@@ -134,6 +134,8 @@
- 
- def MergePermutations : NativeCodeCall<"getMergedTransposePermutation($_builder, $0, $1)">;
- 
-+def MergeDiscardableAttributes : NativeCodeCall<"mergeDiscardableAttributes($0, $1)">;
-+
- def StableHLO_ConvertOpWithShape : NativeCodeCall<
-     "stablehlo::ConvertOp::create($_builder, $_loc, $0.getType(), $1)">;
- 
-@@ -149,8 +151,9 @@
- // op(cst, X) -> op(X, cst)
- class CanonicalizeConstantToRhs<Op StableHLO_OpType>
-   : Pat<(StableHLO_OpType:$op (StableHLO_ConstantOp:$lhs $value), $rhs),
--        (StableHLO_OpType $rhs, $lhs),
--        [(NotConstantOp $rhs), (CommutativeOp $op)]>;
-+        (StableHLO_OpType:$new_op $rhs, $lhs),
-+        [(NotConstantOp $rhs), (CommutativeOp $op)],
-+        [(MergeDiscardableAttributes $op, $new_op)]>;
- 
- ////////
- // AddOp
-@@ -161,8 +164,9 @@
- 
- // Pattern: add(X, 0) -> X
- def AddOp_RemoveNoop
--  : Pat<(StableHLO_AddOp $lhs, (ConstantLikeMatcher AnyZero:$value)),
--        (replaceWithValue $lhs)>;
-+  : Pat<(StableHLO_AddOp:$op $lhs, (ConstantLikeMatcher AnyZero:$value)),
-+        (replaceWithValue $lhs), [],
-+        [(MergeDiscardableAttributes $op, $lhs)]>;
- 
- ////////
- // AndOp
-@@ -173,13 +177,15 @@
- 
- // Pattern: and(X, 0) -> 0
- def AndOp_FoldToZero
--  : Pat<(StableHLO_AndOp $lhs, (StableHLO_ConstantOp:$zero IntZero:$value)),
--        (replaceWithValue $zero)>;
-+  : Pat<(StableHLO_AndOp:$op $lhs, (StableHLO_ConstantOp:$zero IntZero:$value)),
-+        (replaceWithValue $zero), [],
-+        [(MergeDiscardableAttributes $op, $zero)]>;
- 
- // Pattern: and(X, 1) -> X
- def AndOp_RemoveNoop
--  : Pat<(StableHLO_AndOp $lhs, (StableHLO_ConstantOp:$one IntAllOnes:$value)),
--        (replaceWithValue $lhs)>;
-+  : Pat<(StableHLO_AndOp:$op $lhs, (StableHLO_ConstantOp:$one IntAllOnes:$value)),
-+        (replaceWithValue $lhs), [],
-+        [(MergeDiscardableAttributes $op, $lhs)]>;
- 
- ////////
- // BroadcastInDimOp
-@@ -188,7 +194,8 @@
- def BroadcastInDimOp_RemoveNoop
-   : Pat<(StableHLO_BroadcastInDimOp:$op $operand, IotaDims:$dims),
-         (replaceWithValue $operand),
--        [(TypesEqual $op, $operand)]>;
-+        [(TypesEqual $op, $operand)],
-+        [(MergeDiscardableAttributes $op, $operand)]>;
- 
- // Pattern: broadcast_in_dim(broadcast_in_dim(X, [dimsA...]), [dimsB...])
- //       -> broadcast_in_dim(X, merge(dimsA, dimsB))
-@@ -254,7 +261,8 @@
- def ConvertOp_RemoveNoop
-   : Pat<(StableHLO_ConvertOp:$convert $operand),
-         (replaceWithValue $operand),
--        [(TypesEqual $convert, $operand)]>;
-+        [(TypesEqual $convert, $operand)],
-+        [(MergeDiscardableAttributes $convert, $operand)]>;
- 
- ////////
- // DynamicBroadcastInDimOp
-@@ -441,13 +449,15 @@
- // Multiplication by 0. This fold is not trivial for floats in presence of NaNs,
- // so we currently only enable it for ints.
- def MulOp_FoldToZero
--  : Pat<(StableHLO_MulOp $lhs, (StableHLO_ConstantOp:$zero IntZero:$value)),
--        (replaceWithValue $zero)>;
-+  : Pat<(StableHLO_MulOp:$mul_op $lhs, (StableHLO_ConstantOp:$zero IntZero:$value)),
-+        (replaceWithValue $zero), [],
-+        [(MergeDiscardableAttributes $mul_op, $zero)]>;
- 
- // Pattern: multiply(X, 1i) -> X
- def MulOp_RemoveNoop
--  : Pat<(StableHLO_MulOp $lhs, (StableHLO_ConstantOp AnyOne:$value)),
--        (replaceWithValue $lhs)>;
-+  : Pat<(StableHLO_MulOp:$mul_op $lhs, (StableHLO_ConstantOp AnyOne:$value)),
-+        (replaceWithValue $lhs), [],
-+        [(MergeDiscardableAttributes $mul_op, $lhs)]>;
- 
- ////////
- // OrOp
-@@ -457,13 +467,15 @@
- 
- // Pattern: or(X, 1) -> 1
- def OrOp_FoldToOne
--  : Pat<(StableHLO_OrOp $lhs, (StableHLO_ConstantOp:$one IntAllOnes:$value)),
--        (replaceWithValue $one)>;
-+  : Pat<(StableHLO_OrOp:$op $lhs, (StableHLO_ConstantOp:$one IntAllOnes:$value)),
-+        (replaceWithValue $one), [],
-+        [(MergeDiscardableAttributes $op, $one)]>;
- 
- // Pattern: or(X, 0) -> X
- def OrOp_RemoveNoop
--  : Pat<(StableHLO_OrOp $lhs, (StableHLO_ConstantOp:$zero IntZero:$value)),
--        (replaceWithValue $lhs)>;
-+  : Pat<(StableHLO_OrOp:$op $lhs, (StableHLO_ConstantOp:$zero IntZero:$value)),
-+        (replaceWithValue $lhs), [],
-+        [(MergeDiscardableAttributes $op, $lhs)]>;
- 
- ////////
- // PadOp
-@@ -564,8 +576,9 @@
- 
- // Pattern: subtract(X, 0) -> X
- def SubtractOp_RemoveNoop
--  : Pat<(StableHLO_SubtractOp $lhs, (StableHLO_ConstantOp AnyZero:$value)),
--        (replaceWithValue $lhs)>;
-+  : Pat<(StableHLO_SubtractOp:$op $lhs, (StableHLO_ConstantOp AnyZero:$value)),
-+        (replaceWithValue $lhs), [],
-+        [(MergeDiscardableAttributes $op, $lhs)]>;
- 
- ////////
- // SliceOp
 
